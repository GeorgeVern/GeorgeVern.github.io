@inproceedings{LMCor,
      title={Small Language Models Improve Giants by Rewriting Their Outputs}, 
      author={. Vernikos and A. Bra≈æinskas and J. Adamek and J. Mallinson and A. Severyn and E. Malmi},
      booktitle = "Arxiv",
      year={2023},
      abstract = {Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. 
        Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. 
        Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. 
        In this light, we propose a method to correct LLM outputs without relying on their weights. 
        First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. 
        Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits robustness against different prompts, thereby minimizing the need for extensive prompt engineering. 
        Finally, we showcase that the LMCor can be seamlessly integrated with different LLMs at inference time, serving as a plug-and-play module to improve their performance.}.  

}


@inproceedings{huffman_subword,
    abbr={EAMT},
    title = {Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT.},
    author = {B. Wolleb and R. Silvestri and G. Vernikos  and L. Dolamic and A. Popescu-Belis},
    year = {2023},
    booktitle = "EAMT",
    abstract = {Subword tokenization is the \textit{de facto} standard for tokenization in neural language models and machine translation systems.  
    Three advantages are frequently cited in favor of subwords: shorter encoding of frequent tokens, compositionality of subwords, and ability to deal with unknown words. 
    As their relative importance is not entirely clear yet, we propose a tokenization approach that enables us to separate frequency (the first advantage) from compositionality. 
    The approach uses Huffman coding to tokenize words, by order of frequency, using a fixed amount of symbols. 
    Experiments with CS-DE, EN-FR and EN-DE NMT show that frequency alone accounts for 90\%-95\% of the scores reached by BPE, hence compositionality has less importance than previously thought.},
}

@inproceedings{gpoet,
    abbr={SIGHUM},
    title = {GPoeT: a Language Model Trained for Rhyme Generation on Synthetic Data},
    author = {A. Popescu-Belis and A.R. Atrio and B. Bernath and E. Boisson and T. Ferrari and X. Theimer-lienhard and G. Vernikos},
    year = {2023},
    booktitle = "SIGHUM",
    abstract = {Poem generation with language models requires the modeling of rhyming patterns. We propose a novel solution for learning to rhyme, based on synthetic data generated with a rule-based rhyming algorithm. 
    The algorithm and an evaluation metric use a phonetic dictionary and the definitions of perfect and assonant rhymes. 
    We fine-tune a GPT-2 English model with 124M parameters on 142 MB of natural poems and find that this model generates consecutive rhymes infrequently (11\%). 
    We then fine-tune the model on 6 MB of synthetic quatrains with consecutive rhymes (AABB) and obtain nearly 60\% of rhyming lines in samples generated by the model. 
    Alternating rhymes (ABAB) are more difficult to model because of longer-range dependencies, but they are still learnable from synthetic data, reaching 45\% of rhyming lines in generated samples.},
    pdf = {https://aclanthology.org/2023.latechclfl-1.2/},
    code = {https://github.com/heig-iict-ida/crpo},
}

@inproceedings{document_metric,
    abbr={WMT},
    title = {Embarrassingly Easy Document-Level MT Metrics: How to Convert Any Pretrained Metric Into a Document-Level Metric},
    author = {G. Vernikos and B. Thompson and P. Mathur and M. Federico},
    year = {2022},
    booktitle = "WMT",
    abstract = {We present a very simple method for extending pretrained machine translation metrics to incorporate document-level context. We apply our method to four popular metrics: BERTScore, Prism, COMET, and the reference-free metric COMET-QE. 
    We evaluate our document-level metrics on the MQM annotations from the WMT 2021 metrics shared task and find that the document-level metrics outperform their sentence-level counterparts in about 85\% of the tested conditions, when excluding results on low-quality human references. 
    Additionally, we show that our document-level extension of COMET-QE dramatically improves accuracy on discourse phenomena tasks, supporting our hypothesis that our document-level metrics are resolving ambiguities in the reference sentence by using additional context.},
    pdf = {https://aclanthology.org/2022.wmt-1.6/},
    code = {https://github.com/amazon-science/doc-mt-metrics},
    blog = {https://slator.com/converting-pre-trained-mt-metric-into-document-level-metric-amazon-study/},
    selected={true},
}

@inproceedings{smala,
abbr={EMNLP},
      title={Subword Mapping and Anchoring across Languages},
      author={G. Vernikos and A. Popescu-Belis},
      year={2021},
      booktitle = "EMNLP (Findings)",
      selected={true},
      pdf= {https://aclanthology.org/2021.findings-emnlp.224/},
      abstract = {State-of-the-art multilingual systems rely on shared vocabularies that sufficiently cover all considered languages. 
      To this end, a simple and frequently used approach makes use of subword vocabularies constructed jointly over several languages. 
      We hypothesize that such vocabularies are suboptimal due to false positives (identical subwords with different meanings across languages) and false negatives (different subwords with similar meanings). 
      To address these issues, we propose Subword Mapping and Anchoring across Languages (SMALA), a method to construct bilingual subword vocabularies. 
      SMALA extracts subword alignments using an unsupervised state-of-the-art mapping technique and uses them to create cross-lingual anchors based on subword similarities. 
      We demonstrate the benefits of SMALA for cross-lingual natural language inference (XNLI), where it improves zero-shot transfer to an unseen language without task-specific data, but only by sharing subword embeddings. 
      Moreover, in neural machine translation, we show that joint subword vocabularies obtained with SMALA lead to higher BLEU scores on sentences that contain many false positives and false negatives.},
      code={https://github.com/GeorgeVern/smala},
      slides = "SMALA_Workshop.pdf"
}

@inproceedings{cal,
abbr={EMNLP},
      title={Active Learning by Acquiring Contrastive Examples},
      author={K. Margatina and G. Vernikos
       and L. Barrault and N. Aletras},
      year={2021},
      booktitle = "EMNLP",
      selected= {true},
      abstract = {Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. 
      In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. 
      We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. 
      Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. 
      We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.},
      pdf = {https://aclanthology.org/2021.emnlp-main.51/},
      code={https://github.com/mourga/contrastive-active-learning},
}

@inproceedings{atrio-etal-2021-iict,
abbr={WMT},
    title = "The {IICT}-Yverdon System for the {WMT} 2021 Unsupervised {MT} and Very Low Resource Supervised {MT} Task",
    author = {A.R. Atrio and
      G. Luthier  and
      A. Fahy  and
      G. Vernikos  and
      A. Popescu-Belis  and
      L. Dolamic},
    booktitle = "WMT",
    year = {2021},
    abstract = {In this paper, we present the systems submitted by our team from the Institute of ICT (HEIG-VD / HES-SO) to the Unsupervised MT and Very Low Resource Supervised MT task. 
    We first study the improvements brought to a baseline system by techniques such as back-translation and initialization from a parent model. 
    We find that both techniques are beneficial and suffice to reach performance that compares with more sophisticated systems from the 2020 task. 
    We then present the application of this system to the 2021 task for low-resource supervised Upper Sorbian (HSB) to German translation, in both directions. 
    Finally, we present a contrastive system for HSB-DE in both directions, and for unsupervised German to Lower Sorbian (DSB) translation, which uses multi-task training with various training schedules to improve over the baseline.},
    pdf = {https://aclanthology.org/2021.wmt-1.103},
}

@inproceedings{vernikos-etal-2020-domain,
    abbr={EMNLP},
    title = "{D}omain {A}dversarial {F}ine-{T}uning as an {E}ffective {R}egularizer",
    author = {G. Vernikos and
              K. Margatina and
              A. Chronopoulou and
              I. Androutsopoulos},
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    year = {2020},
    booktitle = "EMNLP (Findings)",
    pages = "3103--3112",
    selected = {true},
    abstract = {In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. 
    However, standard fine-tuning can degrade the general-domain representations captured during pretraining. 
    To address this issue, we introduce a new regularization technique, AFTER; domain Adversarial Fine-Tuning as an Effective Regularizer. 
    Specifically, we complement the task-specific loss used during fine-tuning with an adversarial objective. 
    This additional loss term is related to an adversarial classifier, that aims to discriminate between in-domain and out-of-domain text representations. I
    n-domain refers to the labeled dataset of the task at hand while out-of-domain refers to unlabeled data from a different domain. 
    Intuitively, the adversarial classifier acts as a regularize which prevents the model from overfitting to the task-specific domain. 
    Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning.},
    pdf = {https://aclanthology.org/2020.findings-emnlp.278/},
    code={https://github.com/GeorgeVern/AFTERV1.0},
    slides = "AFTER SustaiNLP.pdf"
}
