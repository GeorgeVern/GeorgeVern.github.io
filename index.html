<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Giorgos Vernikos</title> <meta name="author" content="Giorgos Vernikos"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/vitae/">vitae</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Giorgos</span> Vernikos </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> </div> <div class="clearfix"> <p>Hello! I am a final-year Ph.D. student at the Electrical Engineering department at <a href="https://www.epfl.ch/en/" rel="external nofollow noopener" target="_blank">EPFL</a> and a research assistant at <a href="https://heig-vd.ch/" rel="external nofollow noopener" target="_blank">HEIG-VD</a>, in Switzerland. My Ph.D. advisor is <a href="http://iict-space.heig-vd.ch/apu/" rel="external nofollow noopener" target="_blank">Andrei Popescu-Belis</a>. I am interested in machine translation and large language models. I also have a growing interest in machine translation evaluation.</p> <p>During my Ph.D. I interned at Google in the <a href="https://assistant.google.com/" rel="external nofollow noopener" target="_blank">Google Assistant</a> team in Zurich, Switzerland. I also did an internship at Amazon Web Services (<a href="https://aws.amazon.com/translate/" rel="external nofollow noopener" target="_blank">AWS</a>) in Santa Clara, CA, working with the Amazon Translate team.</p> <p>Before coming to Switzerland, I obtained my diploma (combined BEng and MEng) in <a href="https://www.ece.ntua.gr/en" rel="external nofollow noopener" target="_blank">Electrical and Computer Engineering</a> and my MSc in Data Science and Machine Learning from the <a href="https://www.ntua.gr/en/" rel="external nofollow noopener" target="_blank">National Technical University of Athens in Athens</a>, Greece. In my Master’s thesis, I worked on <em>Adversarial Fine-Tuning of Pretrained Language Models</em> under the supervision of <a href="https://www.semanticscholar.org/author/A.-Stafylopatis/1684529?sort=pub-date" rel="external nofollow noopener" target="_blank">Andreas Stafylopatis</a>. During that time I was also working as a Machine Learning Engineer at <a href="https://www.deepsea.ai/" rel="external nofollow noopener" target="_blank">DeepSea Technologies</a>.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">May 16, 2024</th> <td> Excited to announce that our paper <a href="https://aclanthology.org/2024.acl-long.653/" rel="external nofollow noopener" target="_blank">Don’t Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation</a> has been accepted at the main conference of <a href="https://2024.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL 2024</a>! </td> </tr> <tr> <th scope="row">Feb 21, 2024</th> <td> Invited talks at <a href="https://ist-unbabel-seminars.github.io/index" rel="external nofollow noopener" target="_blank">IST &amp; Unbabel Seminars</a> and Microsoft MT reading group! </td> </tr> <tr> <th scope="row">Jan 18, 2024</th> <td> My Google internship project <a href="https://arxiv.org/abs/2305.13514" rel="external nofollow noopener" target="_blank">Small Language Models Improve Giants by Rewriting Their Outputs</a> has been accepted at <a href="https://2024.eacl.org/" rel="external nofollow noopener" target="_blank">EACL 2024</a>! </td> </tr> <tr> <th scope="row">Jan 16, 2024</th> <td> The preprint for our work <a href="https://arxiv.org/abs/2401.06688" rel="external nofollow noopener" target="_blank">Don’t Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation</a> is available on Arxiv! </td> </tr> <tr> <th scope="row">Jul 14, 2023</th> <td> Invited talk at <a href="https://www.archimedesai.gr/en/two-day-workshop-on-natural-language-processing" rel="external nofollow noopener" target="_blank">Archimedes 2023 Summer NLP Workshop</a> in Athens! </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ACL</abbr> </div> <div id="QE-fusion" class="col-sm-8"> <div class="title">Don’t Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation</div> <div class="author"> <em>G. Vernikos</em>, and <a href="https://scholar.google.com/citations?user=E4kF67wAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">A. Popescu-Belis</a> </div> <div class="periodical"> <em>In Proceedings of the 62st Annual Meeting of the Association for Computational Linguistics</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2024.acl-long.653/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://twitter.com/gvernikos/status/1746907975028035756" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">TL;DR</a> <a href="https://github.com/GeorgeVern/qe-fusion" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generates novel translations in over half of the cases and consistently outperforms other methods across varying numbers of candidates (5-200). Furthermore, we empirically establish that QE-fusion scales linearly with the number of candidates in the pool. QE-fusion proves effective in enhancing LLM-based translation without the need for costly retraining of LLMs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">EACL</abbr> <p> <i class="em em-trophy" aria-role="presentation" style="font-size: 0.7em;" aria-label="TROPHY"></i> <span style="font-size: 0.8em;"> 🎤 Oral</span> </p> </div> <div id="LMCor" class="col-sm-8"> <div class="title">Small Language Models Improve Giants by Rewriting Their Outputs</div> <div class="author"> <em>G. Vernikos</em>, <a href="http://www.abrazinskas.com/" rel="external nofollow noopener" target="_blank">A. Bražinskas</a>, <a href="https://www.semanticscholar.org/author/Jakub-Adamek/50290651" rel="external nofollow noopener" target="_blank">J. Adamek</a>, <a href="https://scholar.google.co.uk/citations?user=6Yjwg9EAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">J. Mallinson</a>, <a href="https://scholar.google.com/citations?user=EoVDI3MAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">A. Severyn</a>, and <a href="https://ericmalmi.com/" rel="external nofollow noopener" target="_blank">E. Malmi</a> </div> <div class="periodical"> <em>In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2024.eacl-long.165/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://twitter.com/gvernikos/status/1661290757385330688" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">TL;DR</a> <a href="https://github.com/GeorgeVern/lmcor" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/EACL24_lmcor_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/lmcor_eacl.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. In this light, we propose a method to correct LLM outputs without relying on their weights. First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits robustness against different prompts, thereby minimizing the need for extensive prompt engineering. Finally, we showcase that the LMCor can be seamlessly integrated with different LLMs at inference time, serving as a plug-and-play module to improve their performance.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">WMT</abbr> <p> <i class="em em-trophy" aria-role="presentation" style="font-size: 0.7em;" aria-label="TROPHY"></i> <span style="font-size: 0.8em;"> 🎤 Oral</span> </p> </div> <div id="document_metric" class="col-sm-8"> <div class="title">Embarrassingly Easy Document-Level MT Metrics: How to Convert Any Pretrained Metric Into a Document-Level Metric</div> <div class="author"> <em>G. Vernikos</em>, <a href="https://scholar.google.com/citations?user=6IGa-LoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">B. Thompson</a>, <a href="https://scholar.google.com/citations?user=QgWJzakAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">P. Mathur</a>, and <a href="https://scholar.google.com/citations?user=WaGw_qYAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">M. Federico</a> </div> <div class="periodical"> <em>In Proceedings of the Seventh Conference on Machine Translation</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.wmt-1.6/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://slator.com/converting-pre-trained-mt-metric-into-document-level-metric-amazon-study/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://twitter.com/gvernikos/status/1575499992848302081" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">TL;DR</a> <a href="https://github.com/amazon-science/doc-mt-metrics" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>We present a very simple method for extending pretrained machine translation metrics to incorporate document-level context. We apply our method to four popular metrics: BERTScore, Prism, COMET, and the reference-free metric COMET-QE. We evaluate our document-level metrics on the MQM annotations from the WMT 2021 metrics shared task and find that the document-level metrics outperform their sentence-level counterparts in about 85% of the tested conditions, when excluding results on low-quality human references. Additionally, we show that our document-level extension of COMET-QE dramatically improves accuracy on discourse phenomena tasks, supporting our hypothesis that our document-level metrics are resolving ambiguities in the reference sentence by using additional context.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">EMNLP</abbr> </div> <div id="smala" class="col-sm-8"> <div class="title">Subword Mapping and Anchoring across Languages</div> <div class="author"> <em>G. Vernikos</em>, and <a href="https://scholar.google.com/citations?user=E4kF67wAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">A. Popescu-Belis</a> </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: EMNLP 2021</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2021.findings-emnlp.224/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://twitter.com/gvernikos/status/1438135806577758212" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">TL;DR</a> <a href="https://github.com/GeorgeVern/smala" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/SMALA_Workshop.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>State-of-the-art multilingual systems rely on shared vocabularies that sufficiently cover all considered languages. To this end, a simple and frequently used approach makes use of subword vocabularies constructed jointly over several languages. We hypothesize that such vocabularies are suboptimal due to false positives (identical subwords with different meanings across languages) and false negatives (different subwords with similar meanings). To address these issues, we propose Subword Mapping and Anchoring across Languages (SMALA), a method to construct bilingual subword vocabularies. SMALA extracts subword alignments using an unsupervised state-of-the-art mapping technique and uses them to create cross-lingual anchors based on subword similarities. We demonstrate the benefits of SMALA for cross-lingual natural language inference (XNLI), where it improves zero-shot transfer to an unseen language without task-specific data, but only by sharing subword embeddings. Moreover, in neural machine translation, we show that joint subword vocabularies obtained with SMALA lead to higher BLEU scores on sentences that contain many false positives and false negatives.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">EMNLP</abbr> <p> <i class="em em-trophy" aria-role="presentation" style="font-size: 0.7em;" aria-label="TROPHY"></i> <span style="font-size: 0.8em;"> 🎤 Oral</span> </p> </div> <div id="cal" class="col-sm-8"> <div class="title">Active Learning by Acquiring Contrastive Examples</div> <div class="author"> <a href="https://scholar.google.com/citations?user=517t5gEAAAAJ" rel="external nofollow noopener" target="_blank">K. Margatina</a>, <em>G. Vernikos</em>, <a href="https://scholar.google.fr/citations?user=i4IBjw4AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">L. Barrault</a>, and <a href="https://scholar.google.co.uk/citations?user=uxRWFhoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">N. Aletras</a> </div> <div class="periodical"> <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2021.emnlp-main.51/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://twitter.com/katemargatina/status/1437393852227276801" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">TL;DR</a> <a href="https://github.com/mourga/contrastive-active-learning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%67%65%6F%72%67%69%6F%73.%76%65%72%6E%69%6B%6F%73@%65%70%66%6C.%63%68" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=1AgA0_YAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/1972392392" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/GeorgeVern" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/giorgos-vernikos-771200151" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/gvernikos" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Giorgos Vernikos. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>